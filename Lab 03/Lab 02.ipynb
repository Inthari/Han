{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaffb5e0-e929-42e1-a390-640f188b51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stopwords, csv\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fba276a7-1d84-4151-b8f6-4d6284ec6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HelperClear(sentences):\n",
    "  \"\"\"\n",
    "  Helper to Clear Your Sentences\n",
    "  \"\"\"\n",
    "  output = sentences.lower().split()\n",
    "  unlist = stopwords.get_stopwords(\"english\")\n",
    "  output = [i for i in output if i not in unlist]\n",
    "  result = \" \".join(output)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b0ed36-0cbe-486b-925b-e88f1f01a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlfile = \"data/bbc-text.csv\"\n",
    "\n",
    "sentences = []; labels = []\n",
    "\n",
    "with open(urlfile, 'r') as urli:\n",
    "  reader = csv.reader(urli, delimiter=\",\")\n",
    "  next(reader)\n",
    "  for input in reader:\n",
    "    labels.append(input[0])\n",
    "    sentences.append(HelperClear(input[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511e413d-18cb-4fa7-818c-cded6311bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalToken = 1000\n",
    "\n",
    "OutputLength = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a7504ae-f7ac-4b5f-86c6-4dd165fce971",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(0.8 * len(sentences))\n",
    "\n",
    "train_input = sentences[:train_split]\n",
    "\n",
    "validate_input = sentences[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0ca3e4-252f-4a48-b4f6-fff059f7ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=TotalToken, output_sequence_length=OutputLength)\n",
    "\n",
    "vectorizer.adapt(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a098f68-2a35-40d6-8beb-8a143e608430",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = labels[:train_split]\n",
    "\n",
    "validate_label = labels[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04de121-8a2a-4218-bafe-a7ab47316a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = tf.keras.layers.StringLookup(num_oov_indices=0)\n",
    "\n",
    "label_encoder.adapt(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae8f115-bc2d-4bc9-ab17-84939e9abdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([4, 1, 0, 0, 3])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label = label_encoder(train_label)\n",
    "\n",
    "validate_label = label_encoder(validate_label)\n",
    "\n",
    "train_label[:5].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c31ff5-3284-4212-a366-4241f412d8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([3, 4, 2, 0, 0])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_label[:5].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "744039b6-5e98-44ec-839f-e6ebbdd58a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = vectorizer(train_input)\n",
    "\n",
    "validate_input = vectorizer(validate_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99753b94-3fee-468c-a408-1479e5e44ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([ \n",
    "  tf.keras.layers.Embedding(TotalToken, 16, input_length=OutputLength),\n",
    "  tf.keras.layers.GlobalAveragePooling1D(),\n",
    "  tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "loss = \"sparse_categorical_crossentropy\"\n",
    "\n",
    "model.compile(loss=loss, optimizer=\"adam\", metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9ee81-e278-44be-90d4-f5e98cb475ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_input, train_label, epochs=15, validation_data=(validate_input, validate_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52148927",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_input, train_label, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e61ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(validate_input, validate_label, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
